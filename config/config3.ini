; Network with 5 hidden layers. One output layer before the softmax layer.
; layer 0: 400 inputs, 100 outputs, relu activation
; layer 1: 100 inputs, 100 outputs, sigmoid activation
; layer 2: 100 inputs, 100 outputs, sigmoid activation
; layer 3: 100 inputs, 50 outputs, sigmoid activation
; layer 4: 50 inputs, 50 outputs, sigmoid activation
; layer 5: 50 inputs, 4 outputs, sigmoid activation (output layer)
; softmax layer with cross entropy as loss function

[GLOBALS]
show_images=false
image_size=20
min_width=10
max_width=20
min_height=10
max_height=20
noise=0.03
centered=false
data_set_size=800
train_size=0.7
valid_size=0.2
;test_size implied to be 1 - (train_size + valid_size)

epochs=100
batch_size=20
verbose=true

loss=cross_entropy
lrate=0.1
wreg=0.001
wrt=l2
softmax=true
input=400

[LAYER0]
size=100
activation=relu
wr_start=-0.5
wr_end=0.5
br_start=-0.5
br_end=0.5

[LAYER1]
size=100
activation=sigmoid
wr_start=-0.5
wr_end=0.5
br_start=-0.5
br_end=0.5

[LAYER2]
size=100
activation=sigmoid
wr_start=-0.5
wr_end=0.5
br_start=-0.5
br_end=0.5

[LAYER3]
size=50
activation=sigmoid
wr_start=-0.5
wr_end=0.5
br_start=-0.5
br_end=0.5

[LAYER4]
size=50
activation=sigmoid
wr_start=-0.5
wr_end=0.5
br_start=-0.5
br_end=0.5

[LAYER_LAST]
size=4
activation=sigmoid
wr_start=-0.5
wr_end=0.5
br_start=-0.5
br_end=0.5