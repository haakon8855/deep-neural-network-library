; Network with 3 hidden layers. One output layer before the softmax layer.
; layer 0: 400 inputs, 100 outputs, relu activation
; layer 1: 100 inputs, 75 outputs, tanh activation
; layer 2: 75 inputs, 50 outputs, linear activation
; layer 3: 50 inputs, 4 outputs, sigmoid activation
; softmax layer with cross entropy as loss function

[GLOBALS]
show_images=true
image_size=20
min_width=10
max_width=20
min_height=10
max_height=20
noise=0.05
centered=false
data_set_size=500
train_size=0.7
valid_size=0.2
;test_size implied to be 1 - (train_size + valid_size)

epochs=100
batch_size=20
verbose=true

loss=cross_entropy
lrate=0.1
wreg=0.001
wrt=l2
softmax=true
input=400

[LAYER0]
size=100
activation=relu
wr_start=-0.5
wr_end=0.5
br_start=-0.5
br_end=0.5

[LAYER1]
size=75
activation=tanh
wr_start=-0.5
wr_end=0.5
br_start=-0.5
br_end=0.5

[LAYER2]
size=50
activation=linear
lrate=0.08
wr_start=-0.5
wr_end=0.5
br_start=-0.5
br_end=0.5

[LAYER_LAST]
size=4
activation=sigmoid
wr_start=-0.5
wr_end=0.5
br_start=-0.5
br_end=0.5